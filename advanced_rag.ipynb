{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Exercise\n",
    "\n",
    "This notebook is designed as an exercise to build a complete Retrieval-Augmented Generation (RAG) system. In this exercise, you will integrate three main components into a single pipeline:\n",
    "\n",
    "1. **Retrieval Module** – Retrieve relevant documents based on a query.\n",
    "2. **Transformation Module** – Transform the retrieved queries.\n",
    "3. **Generation Module and Evaluation** – Use the transformed data to generate responses and evaluate the overall system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Building the RAG Pipeline\n",
    "\n",
    "Load the data and store it in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'   Einsatz von KI-Agenten zur Automatisierung einf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter: 2000 characters per chunk with an overlap of 200 characters\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 13\n",
      "Preview of the first chunk: Einsatz von KI-Agenten zur Automatisierung einfacher Sachbearbeitungsaufgaben in Unternehmen   Disposition   Eingereicht bei der ZHAW School of Management and Law (SML) Philipp Stalder   Wissenschaftl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an embedding model\n",
    "Use the SentenceTransfomer wrapper as we have done so far.\n",
    "Models are found here: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "or on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "chunk_embeddings = model.encode(chunks, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Index and save index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 13\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a retriever function\n",
    "\n",
    "arguments: query, k, index, chunks, embedding model\n",
    "\n",
    "return: retrieved texts, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_texts(query, k, index, token_split_texts, model):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks and their embeddings for a given query.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    return retrieved_texts, distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build an answer function\n",
    "Build an answer function that takes a query, k, an index and the chunks.\n",
    "\n",
    "return: answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, k, index,texts):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks for the given query using the retriever,\n",
    "    inject them into a prompt, and send it to the Groq LLM to obtain an answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The user's query.\n",
    "    - k (int): Number of retrieved documents to use.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - answer (str): The answer generated by the LLM.\n",
    "    \"\"\"\n",
    "    # Retrieve the top k documents using your retriever function.\n",
    "    # This retriever uses the following definition:\n",
    "    # def retrieve(query, k):\n",
    "    #     query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    #     distances, indices = index.search(query_embedding, k)\n",
    "    #     retrieved_texts = [token_split_texts[i] for i in indices[0]]\n",
    "    #     retrieved_embeddings = np.array([chunk_embeddings[i] for i in indices[0]])\n",
    "    #     return retrieved_texts, retrieved_embeddings, distances\n",
    "    model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    retrieved_texts, _ = retrieve_texts(query, k, index, texts, model)\n",
    "    \n",
    "    # Combine the retrieved documents into a single context block.\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # Build a prompt that instructs the LLM to answer the query based on the context.\n",
    "    prompt = (\n",
    "        \"Answer the following question using the provided context. \"\n",
    "        \"Explain it as if you are explaining it to a 5 year old.\\n\\n\"\n",
    "        \"Context:\\n\" + context + \"\\n\\n\"\n",
    "        \"Question: \" + query + \"\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the Groq client and send the prompt.\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    \n",
    "    # Extract and return the answer.\n",
    "    answer = llm.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer: You didn't ask a question about asthma, so I'll answer a different question. \n",
      "\n",
      "Let me explain something to you in a simple way. Imagine you have a lemonade stand, and you want to know how to make it better. You would ask people who run lemonade stands how they do it, right? That's kind of like what the text is talking about. It's about asking people who work in companies how they use special computer programs to make their work easier. These programs are called KI-Agenten, which is like a robot that helps people do their jobs. The text is trying to understand how these robots can help people work more efficiently and make their jobs easier. \n",
      "\n",
      "So, to answer a question that you didn't ask, the most important thing is to ask people who work in companies how they use these special computer programs, and then use that information to make their work better. Isn't that cool?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "answer = answer_query(query, 5, index, chunks)\n",
    "print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Rewriter\n",
    "\n",
    "Take a query and an api key for the model and rewrite the query. \n",
    "\n",
    "Rewriting a query: A Language Model is prompted to rewrite a query to better suit a task.\n",
    "\n",
    "Other Transfomrations are implemented in a similar fashion, this is just an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(query, groq_api_key):\n",
    "    \"\"\"\n",
    "    Rewrite the user's query into to potentially improve retrieval.\n",
    "    Parameters:\n",
    "    - query (str): The original user query.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - rewritten_query (str): The rewritten query.\n",
    "    \"\"\"\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "    # Build a prompt for rewriting the query\n",
    "    rewriting_prompt = (\n",
    "        \"Rewrite the following query into a format, such that it can be answered by looking at medical guidelines. \"\n",
    "        \"Keep the keywords but ensure that it is close to a format, such as in medical guidelines. Just answer with the rewritten query\\n\\n\"\n",
    "        \"Query: \" + query\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rewriting_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Use the same model (for example, llama) to perform query rewriting\n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "    )\n",
    "    rewritten_query = llm.choices[0].message.content.strip()\n",
    "    return rewritten_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement the rewriter into your answer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_with_rewriting(query, k, index, texts, groq_api_key):\n",
    "    \"\"\"\n",
    "    Retrieve the top k similar text chunks for the given query using a retrieval method\n",
    "    with query rewriting, inject them into a prompt, and send it to the Groq LLM (using llama)\n",
    "    to obtain an answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The user's query.\n",
    "    - k (int): Number of retrieved documents to use.\n",
    "    - index: The FAISS index.\n",
    "    - texts (list): The tokenized text chunks mapping.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - answer (str): The answer generated by the LLM.\n",
    "    \"\"\"\n",
    "    model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    # Use the new retrieval function with query rewriting.\n",
    "    rewritten_query = rewrite_query(query, groq_api_key)    \n",
    "    print(\"Rewritten Query:\", rewritten_query) ## FYI\n",
    "\n",
    "    retrieved_texts, _ = retrieve_texts(rewritten_query, k, index, texts, model)\n",
    "    \n",
    "    # Combine the retrieved documents into a single context block.\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # Build a prompt that instructs the LLM to answer the query based on the context.\n",
    "    prompt = (\n",
    "        \"Answer the following question using the provided context. \"\n",
    "        \"Explain it as if you are explaining it to a 5 year old.\\n\\n\"\n",
    "        \"Context:\\n\" + context + \"\\n\\n\"\n",
    "        \"Question: \" + query + \"\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    \n",
    "    # Initialize the Groq client and send the prompt.\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    llm = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.3-70b-versatile\"\n",
    "    )\n",
    "    \n",
    "    # Extract and return the answer.\n",
    "    answer = llm.choices[0].message.content\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten Query: What are the key diagnostic criteria for asthma, and what is the primary factor that guides the diagnosis of asthma in clinical practice?\n",
      "LLM Answer: I think there might be some confusion! The text provided doesn't mention anything about diagnosing asthma. It talks about the use of artificial intelligence (AI) and automation in administrative tasks, and how it can help free up human resources for more strategic and creative work.\n",
      "\n",
      "Let me try to explain it in a way that a 5-year-old can understand: Imagine you have a lot of toys to put away in your toy box. You can either do it yourself, which might take a long time, or you can use a special machine that can help you do it faster and more efficiently. That's kind of like what AI and automation do for grown-ups in their work. They help with tasks that are repetitive or boring, so that humans can focus on more important and fun things!\n",
      "\n",
      "But, if you're looking for information about diagnosing asthma, I'd be happy to help with that too! Just let me know what you're looking for, and I'll do my best to explain it in a way that's easy to understand.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "answer = answer_query_with_rewriting(query, 5, index, chunks, groq_api_key)\n",
    "print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 .Evaluation\n",
    "\n",
    "Select random chunks from all your chunks, and generate a question to each of these chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import httpx  # Ensure you're catching the correct timeout exception\n",
    "from openai import OpenAI\n",
    "def generate_questions_for_random_chunks(chunks, num_chunks=20, max_retries=3):\n",
    "    \"\"\"\n",
    "    Randomly selects a specified number of text chunks from the provided list,\n",
    "    then generates a question for each selected chunk using the Groq LLM.\n",
    "\n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - num_chunks (int): Number of chunks to select randomly (default is 20).\n",
    "\n",
    "    Returns:\n",
    "    - questions (list of tuples): Each tuple contains (chunk, generated_question).\n",
    "    \"\"\"\n",
    "    # Randomly select the desired number of chunks.\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    \n",
    "    # Initialize the Groq client once\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    questions = []\n",
    "    for chunk in tqdm.tqdm(selected_chunks):\n",
    "        # Build a prompt that asks the LLM to generate a question based on the chunk.\n",
    "        prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        generated_question = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Try calling the API with simple retry logic.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                     model=\"gpt-4o-mini\",\n",
    "                    messages=messages\n",
    "                )\n",
    "                generated_question = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred for chunk. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)  # Wait a bit before retrying.\n",
    "        \n",
    "        # If all attempts fail, use an error message as the generated question.\n",
    "        if generated_question is None:\n",
    "            generated_question = \"Error: Failed to generate question after several retries.\"\n",
    "        \n",
    "        questions.append((chunk, generated_question))\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "effizient und qualitativ hochwertig zu automatisieren. So konnten Bulander et al. (2022) feststellen...\n",
      "Generated Question: How do recent advancements in Robotic Process Automation (RPA) and Large Language Models (LLMs) impact the efficiency and quality of administrative tasks, and what implications do these technologies have on the division of labor between humans and machines?\n",
      "\n",
      "Chunk 2:\n",
      "wissenschaftlicher als auch aus praktischer Sicht enorm. Aus wissenschaftlicher Perspektive existier...\n",
      "Generated Question: What are the key benefits and challenges that businesses face when integrating AI agents and Robotic Process Automation into their administrative processes, and how does employee acceptance play a role in this transition?\n",
      "\n",
      "Chunk 3:\n",
      "9 5.2 UNTERSUCHUNGSOBJEKTE UND STICHPROBENAUSWAHL ................................................. ...\n",
      "Generated Question: What are the current and future potentials of Robotic Process Automation and Artificial Intelligence in transforming organizational management and work practices, as discussed in the related literature?\n",
      "\n",
      "Chunk 4:\n",
      "Einsatz von KI-Agenten zur Automatisierung einfacher Sachbearbeitungsaufgaben in Unternehmen   Dispo...\n",
      "Generated Question: What are the potential applications and implications of using AI agents for automating simple administrative tasks within businesses, as discussed in Sandro Uhler's research on the topic?\n",
      "\n",
      "Chunk 5:\n",
      "Verwaltungsarbeiten anhand von Large Language Models am Beispiel von ChatGPT. Anwendungen und Konzep...\n",
      "Generated Question: How can AI agents, like those powered by Large Language Models, be effectively implemented in businesses to enhance efficiency in administrative tasks, considering the technological prerequisites and organizational challenges?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions = generate_questions_for_random_chunks(chunks, num_chunks=5, max_retries=2)\n",
    "for idx, (chunk, question) in enumerate(questions, start=1):\n",
    "    print(f\"Chunk {idx}:\\n{chunk[:100]}...\\nGenerated Question: {question}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Test the questions with your built retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generated_questions(question_tuples, k, index, texts, groq_api_key):\n",
    "    \"\"\"\n",
    "    For each (chunk, generated_question) tuple in the provided list, use the prebuilt\n",
    "    retrieval function to generate an answer for the generated question. The function\n",
    "    returns a list of dictionaries containing the original chunk, the generated question,\n",
    "    and the answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - question_tuples (list of tuples): Each tuple is (chunk, generated_question)\n",
    "    - k (int): Number of retrieved documents to use for answering.\n",
    "    - index: The FAISS index.\n",
    "    - texts (list): The tokenized text chunks mapping.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - results (list of dict): Each dict contains 'chunk', 'question', and 'answer'.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for chunk, question in question_tuples:\n",
    "        # Use your retrieval-based answer function. Here we assume the function signature is:\n",
    "        # answer_query(query, k, index, texts, groq_api_key)\n",
    "        answer = answer_query(question, k, index, texts) #query, k, index,texts\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Preview: effizient und qualitativ hochwertig zu automatisieren. So konnten Bulander et al. (2022) feststellen\n",
      "Generated Question: How do recent advancements in Robotic Process Automation (RPA) and Large Language Models (LLMs) impact the efficiency and quality of administrative tasks, and what implications do these technologies have on the division of labor between humans and machines?\n",
      "Answer: Oh boy, let's talk about robots and machines. You know how sometimes we have to do boring tasks like fill out forms or do the same thing over and over again? Well, there are special machines called Robotic Process Automation (RPA) and Large Language Models (LLMs) that can help us with those tasks.\n",
      "\n",
      "These machines are like super smart helpers that can do things faster and more accurately than humans. They can even learn from us and get better at their jobs. This means that we can use them to help us with boring tasks, and we can focus on more important and fun things.\n",
      "\n",
      "Imagine you have a lemonade stand, and you have to make a lot of lemonade every day. You can use a machine to help you make the lemonade, so you can focus on selling it to people and making them happy. That's kind of what RPA and LLMs do, but instead of making lemonade, they help us with administrative tasks like filling out forms or answering emails.\n",
      "\n",
      "But, just like how you need to tell the machine how to make the lemonade, we need to teach the RPA and LLMs how to do their jobs. And, just like how you need to make sure the machine is working properly, we need to make sure the RPA and LLMs are working correctly and not making mistakes.\n",
      "\n",
      "So, what does this mean for us? It means that we can use these machines to help us with boring tasks, and we can focus on more important and fun things. It also means that we need to learn how to work with these machines, and how to make sure they are working correctly. It's like having a new friend that can help us, but we need to teach them how to play with us.\n",
      "\n",
      "And, just like how you might need to change how you do things when you get a new machine, we might need to change how we do things when we start using RPA and LLMs. But, that's okay, because it can help us get better and more efficient, and we can focus on doing more fun and important things.\n",
      "-----------------------------\n",
      "Chunk Preview: wissenschaftlicher als auch aus praktischer Sicht enorm. Aus wissenschaftlicher Perspektive existier\n",
      "Generated Question: What are the key benefits and challenges that businesses face when integrating AI agents and Robotic Process Automation into their administrative processes, and how does employee acceptance play a role in this transition?\n",
      "Answer: Little buddy, imagine you have a lot of toys to put away in your room. It can be boring and take a long time, right? That's kind of like what happens in big companies with lots of paperwork and tasks that need to be done over and over. But, just like how you might want a robot to help you clean your room, companies can use special computer programs called AI agents and Robotic Process Automation to help them with these tasks.\n",
      "\n",
      "The good news is that these AI agents and robots can do the boring tasks very quickly and accurately, which means the company can save time and money. It's like having a super efficient helper! Also, when people in the company don't have to do the boring tasks, they can focus on more fun and important things, like being creative or helping customers.\n",
      "\n",
      "However, there are some challenges. Sometimes the computers and robots need to learn how to do the tasks, and that can take time. Also, some people in the company might be worried that the robots will take their jobs, which can make them unhappy. That's why it's very important for the company to explain to everyone how the AI agents and robots will help them and make their jobs better.\n",
      "\n",
      "The people in charge of the company need to make sure that everyone understands how the AI agents and robots will work and how they will help the company. They also need to teach the employees new skills so they can work well with the computers and robots. If everyone is happy and understands how the AI agents and robots will help, then the company can be more efficient and successful!\n",
      "-----------------------------\n",
      "Chunk Preview: 9 5.2 UNTERSUCHUNGSOBJEKTE UND STICHPROBENAUSWAHL ................................................. \n",
      "Generated Question: What are the current and future potentials of Robotic Process Automation and Artificial Intelligence in transforming organizational management and work practices, as discussed in the related literature?\n",
      "Answer: Oh boy, let's talk about robots and computers helping us at work. So, you know how we have to do lots of tasks at work that can be boring and take a lot of time? Like filing papers or answering the same questions over and over? Well, there are special machines called Robotic Process Automation (RPA) and Artificial Intelligence (AI) that can help us with those tasks.\n",
      "\n",
      "These machines are like super-smart robots that can do things like:\n",
      "\n",
      "* Help with repetitive tasks, like copying and pasting information\n",
      "* Answer questions and help with customer service\n",
      "* Look at data and make decisions faster and more accurately than humans\n",
      "* Even help managers make better decisions by looking at lots of data\n",
      "\n",
      "The good thing about these machines is that they can help us do our jobs better and faster, and even do some tasks that we don't like to do. This can make our work more fun and interesting, because we can focus on more important and creative tasks.\n",
      "\n",
      "But, some people might be worried that these machines will take their jobs away. That's not exactly true. While machines can do some tasks, they can't do everything. They need humans to help them and make sure they're working correctly.\n",
      "\n",
      "So, in the future, we might see more and more machines helping us at work. This can be a good thing, because it can make our jobs easier and more fun. And, who knows, maybe one day we'll have robots that can help us with even more tasks, like making coffee or doing our laundry!\n",
      "\n",
      "The important thing is that we need to learn how to work with these machines and make sure they're helping us, not hurting us. That way, we can make our work better and more fun, and have more time to do the things we love.\n",
      "-----------------------------\n",
      "Chunk Preview: Einsatz von KI-Agenten zur Automatisierung einfacher Sachbearbeitungsaufgaben in Unternehmen   Dispo\n",
      "Generated Question: What are the potential applications and implications of using AI agents for automating simple administrative tasks within businesses, as discussed in Sandro Uhler's research on the topic?\n",
      "Answer: Kleine Freunde, stell dir vor, du hast einen Helfer, der alle langweiligen und wiederkehrenden Aufgaben für dich erledigt. Das ist genau, was KI-Agenten für Unternehmen tun können. Sie können helfen, einfache administrative Aufgaben zu automatisieren, wie zum Beispiel Daten einzugeben oder Rechnungen zu bearbeiten.\n",
      "\n",
      "Das ist großartig, weil es den Mitarbeitern ermöglicht, sich auf wichtigere und interessantere Aufgaben zu konzentrieren. Es kann auch dazu beitragen, dass die Arbeit effizienter und genauer wird, weil KI-Agenten weniger Fehler machen als Menschen.\n",
      "\n",
      "Aber es gibt auch Herausforderungen. Einige Menschen könnten sich Sorgen machen, dass KI-Agenten ihre Jobs übernehmen könnten. Und es ist wichtig, dass die Unternehmen sicherstellen, dass ihre Mitarbeiter die notwendigen Fähigkeiten haben, um mit KI-Agenten zu arbeiten.\n",
      "\n",
      "Sandro Uhler hat recherchiert, dass KI-Agenten sehr nützlich sein können, um administrative Aufgaben zu automatisieren. Er hat auch herausgefunden, dass es wichtig ist, die Mitarbeiter zu unterstützen, wenn sie mit KI-Agenten arbeiten, und dass die Unternehmen sicherstellen müssen, dass sie die notwendigen Fähigkeiten haben, um diese Technologie zu nutzen.\n",
      "\n",
      "Insgesamt kann die Verwendung von KI-Agenten in Unternehmen sehr vorteilhaft sein, um die Arbeit zu vereinfachen und die Effizienz zu verbessern. Es ist jedoch wichtig, dass die Unternehmen vorsichtig und verantwortungsvoll mit dieser Technologie umgehen.\n",
      "-----------------------------\n",
      "Chunk Preview: Verwaltungsarbeiten anhand von Large Language Models am Beispiel von ChatGPT. Anwendungen und Konzep\n",
      "Generated Question: How can AI agents, like those powered by Large Language Models, be effectively implemented in businesses to enhance efficiency in administrative tasks, considering the technological prerequisites and organizational challenges?\n",
      "Answer: Oh boy, let's talk about AI and businesses!\n",
      "\n",
      "So, you know how there are lots of tasks in a company that are like, super repetitive and boring? Like, imagine you have to do the same thing over and over again, every day. That's where AI agents, like those powered by Large Language Models, can help!\n",
      "\n",
      "These AI agents can do those repetitive tasks for us, so we can focus on more fun and important things. But, to make it work, we need to make sure that the company has the right technology and that the people who work there are okay with using AI.\n",
      "\n",
      "It's like when you get a new toy, and you need to read the instructions to make it work. Companies need to understand how to use AI agents and make sure they have the right tools to make it happen.\n",
      "\n",
      "Also, sometimes people might be a little scared or worried about using AI, because they don't understand it. So, companies need to explain to their employees what AI is and how it can help them, so they feel comfortable using it.\n",
      "\n",
      "If companies can do all these things, they can use AI agents to make their work more efficient and fun! And that's really cool!\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "results = answer_generated_questions(questions, 5, index, chunks, groq_api_key)\n",
    "\n",
    "for item in results:\n",
    "    print(\"Chunk Preview:\", item['chunk'][:100])\n",
    "    print(\"Generated Question:\", item['question'])\n",
    "    print(\"Answer:\", item['answer'])\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate_answers_binary(results, groq_api_key, max_retries=3):\n",
    "    \"\"\"\n",
    "    Evaluates each answer in the results list using an LLM.\n",
    "    For each result (a dictionary containing 'chunk', 'question', and 'answer'),\n",
    "    it sends an evaluation prompt to the Groq LLM which outputs 1 if the answer is on point,\n",
    "    and 0 if it is missing the point.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (list of dict): Each dict must contain keys 'chunk', 'question', and 'answer'.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - max_retries (int): Maximum number of retries if the API call times out.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A dataframe containing the original chunk, question, answer, and evaluation score.\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    for item in tqdm.tqdm(results, desc=\"Evaluating Answers\"):\n",
    "        # Build the evaluation prompt.\n",
    "        prompt = (\n",
    "            \"Evaluate the following answer to the given question. \"\n",
    "            \"If the answer is accurate and complete, reply with 1. \"\n",
    "            \"If the answer is inaccurate, incomplete, or otherwise not acceptable, reply with 0. \"\n",
    "            \"Do not include any extra text.\\n\\n\"\n",
    "            \"Question: \" + item['question'] + \"\\n\\n\"\n",
    "            \"Answer: \" + item['answer'] + \"\\n\\n\"\n",
    "            \"Context (original chunk): \" + item['chunk'] + \"\\n\\n\"\n",
    "            \"Evaluation (1 for good, 0 for bad):\"\n",
    "        )\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "        \n",
    "        generated_eval = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Retry logic in case of timeouts or errors.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    model=\"4o-mini\"\n",
    "                )\n",
    "                generated_eval = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the retry loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred during evaluation. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Error during evaluation: {e}. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # If no valid evaluation was produced, default to 0.\n",
    "        if generated_eval is None:\n",
    "            generated_eval = \"0\"\n",
    "        \n",
    "        # Convert the response to an integer (1 or 0).\n",
    "        try:\n",
    "            score = int(generated_eval)\n",
    "            if score not in [0, 1]:\n",
    "                score = 0\n",
    "        except:\n",
    "            score = 0\n",
    "        \n",
    "        evaluations.append(score)\n",
    "    \n",
    "    # Add the evaluation score to each result.\n",
    "    for i, item in enumerate(results):\n",
    "        item['evaluation'] = evaluations[i]\n",
    "    \n",
    "    # Create a dataframe for manual review.\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  20%|██        | 1/5 [00:06<00:26,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  40%|████      | 2/5 [00:13<00:19,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  60%|██████    | 3/5 [00:19<00:13,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers:  80%|████████  | 4/5 [00:26<00:06,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 1/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 2/3...\n",
      "Error during evaluation: Error code: 404 - {'error': {'message': 'The model `4o-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying attempt 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Answers: 100%|██████████| 5/5 [00:33<00:00,  6.64s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>evaluation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>effizient und qualitativ hochwertig zu automat...</td>\n",
       "      <td>How do recent advancements in Robotic Process ...</td>\n",
       "      <td>Oh boy, let's talk about robots and machines. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wissenschaftlicher als auch aus praktischer Si...</td>\n",
       "      <td>What are the key benefits and challenges that ...</td>\n",
       "      <td>Little buddy, imagine you have a lot of toys t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9 5.2 UNTERSUCHUNGSOBJEKTE UND STICHPROBENAUSW...</td>\n",
       "      <td>What are the current and future potentials of ...</td>\n",
       "      <td>Oh boy, let's talk about robots and computers ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Einsatz von KI-Agenten zur Automatisierung ein...</td>\n",
       "      <td>What are the potential applications and implic...</td>\n",
       "      <td>Kleine Freunde, stell dir vor, du hast einen H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Verwaltungsarbeiten anhand von Large Language ...</td>\n",
       "      <td>How can AI agents, like those powered by Large...</td>\n",
       "      <td>Oh boy, let's talk about AI and businesses!\\n\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0  effizient und qualitativ hochwertig zu automat...   \n",
       "1  wissenschaftlicher als auch aus praktischer Si...   \n",
       "2  9 5.2 UNTERSUCHUNGSOBJEKTE UND STICHPROBENAUSW...   \n",
       "3  Einsatz von KI-Agenten zur Automatisierung ein...   \n",
       "4  Verwaltungsarbeiten anhand von Large Language ...   \n",
       "\n",
       "                                            question  \\\n",
       "0  How do recent advancements in Robotic Process ...   \n",
       "1  What are the key benefits and challenges that ...   \n",
       "2  What are the current and future potentials of ...   \n",
       "3  What are the potential applications and implic...   \n",
       "4  How can AI agents, like those powered by Large...   \n",
       "\n",
       "                                              answer  evaluation  \n",
       "0  Oh boy, let's talk about robots and machines. ...           0  \n",
       "1  Little buddy, imagine you have a lot of toys t...           0  \n",
       "2  Oh boy, let's talk about robots and computers ...           0  \n",
       "3  Kleine Freunde, stell dir vor, du hast einen H...           0  \n",
       "4  Oh boy, let's talk about AI and businesses!\\n\\...           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_evaluations = evaluate_answers_binary(results, openai_api_key)\n",
    "display(df_evaluations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
