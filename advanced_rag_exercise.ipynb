{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Exercise\n",
    "\n",
    "This notebook is designed as an exercise to build a complete Retrieval-Augmented Generation (RAG) system. In this exercise, you will integrate three main components into a single pipeline:\n",
    "\n",
    "1. **Retrieval Module** – Retrieve relevant documents based on a query.\n",
    "2. **Transformation Module** – Transform the retrieved queries.\n",
    "3. **Generation Module and Evaluation** – Use the transformed data to generate responses and evaluate the overall system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # For generating embeddings for text chunks\n",
    "import faiss\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Building the RAG Pipeline\n",
    "\n",
    "Load the data and store it in a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'   Einsatz von KI-Agenten zur Automatisierung einf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load the pdf from the path\n",
    "glob_path = \"data/*.pdf\"\n",
    "text = \"\"\n",
    "for pdf_path in tqdm.tqdm(glob.glob(glob_path)):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "         # Extract text from all pages in the PDF\n",
    "        text += \" \".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "\n",
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Einsatz von KI-Agenten zur Automatisierung einfacher Sachbearbeitungsaufgaben in Unternehmen   Disposition   Eingereicht bei der ZHAW School of Management and Law (SML) Philipp Stalder   Wissenschaftliche Methoden (2025-FS)   Abgegeben am 13.04.2025 von Sandro Uhler      I Inhaltsverzeichnis INHALTSVERZEICHNIS .............................................................................................................. I LITERATURVERZEICHNIS ....................................................................................................... II ABBILDUNGSVERZEICHNIS .................................................................................................. III 1 ABSTRACT UND KEYWORDS ........................................................................................... 1 2 PROBLEMSTELLUNG UND RELEV ANZ DES THEMAS .............................................. 2 3 STAND DER FORSCHUNG .................................................................................................. 4 3.1 ANWENDUNGSMÖGLICHKEITEN UND POTENZIALE VON KI-AGENTEN .............................. 4 3.2 TECHNOLOGISCHE VORAUSSETZUNGEN UND ORGANISATORISCHE HERAUSFORDERUNGEN  .......................................................................................................................................... 5 3.3 AUSWIRKUNGEN AUF PROZESSE UND ROLLENVERTEILUNG IM UNTERNEHMEN ............... 6 3.4 FORSCHUNGSLÜCKE .......................................................................................................... 6 4 FORSCHUNGSFRAGEN ...................................................................................................... 8 5 FORSCHUNGSDESIGN UND METHODISCHES VORGEHEN .................................... 9 5.1 GEWÄHLTER FORSCHUNGSANSATZ ................................................................................... 9 5.2 UNTERSUCHUNGSOBJEKTE UND STICHPROBENAUSWAHL ................................................. 9 5.3 BESCHREIBUNG DER DATENERHEBUNG ............................................................................. 9 5.4 OPERATIONALISIERUNG UND MESSBARKEIT ................................................................... 10      II Literaturverzeichnis  Bulander, R., Kreuzwieser, S., Kimmig, A., Kölmel, B., & Ovtcharova, J. (2022). Robotic Process Automation und Künstliche Intelligenz: Aktuelle und zukünftige Potenziale von RPA und KI. ERP Management. https://doi.org/10.30844/erp_22-4_44-49 Claus, I., & Szupories, M. (2021). Künstliche Intelligenz und Führung: Automatisierung und die Veränderung von Managementaufgaben und -Arbeitsweisen. Springer. https://doi.org/10.1007/978-3-662-63223-9_18 Petrat, D., Yenice, I., & Bier, L. \\\\AU-S., Ilka. (2022). Akzeptanz einer Künstlichen Intelligenz als organisatorische Führungskraft: Eine Fragebogenstudie. TATuP Zeitschrift für Technikfolgenabschätzung in Theorie und Praxis. https://doi.org/10.14512/tatup.31.2.64 Pfeiffer, S. (2020). Kontext und KI: Zum Potenzial der Beschäftigten für Künstliche Intelligenz und Machine-Learning. HMD Praxis der Wirtschaftsinformatik. https://doi.org/10.1365/s40702-020-00609-8 Rammer, C., Bertschek, I., Schuck, B., Demary, V ., & Goecke, H. (2020). Einsatz von Künstlicher Intelligenz in der Deutschen Wirtschaft: Stand der KI-Nutzung im Jahr 2019. https://www.econstor.eu/handle/10419/222374 Schönberger, M., & Beinke, J. H. (2023). Hybride Intelligenz als Konvergenz menschlicher und künstlicher Intelligenz – Wie verändert ChatGPT die Wissensarbeit? HMD Praxis der Wirtschaftsinformatik. https://doi.org/10.1365/s40702-023-00997-7 Stumpp, S., & Morelli, F. (2023). Automatisierungspotenziale von Büro- und Verwaltungsarbeiten anhand von Large Language Models am Beispiel von ChatGPT. Anwendungen und Konzepte der Wirtschaftsinformatik. https://doi.org/10.26034/lu.akwi.2023.4490 Vo l l e n k e m p e r,  L . ,  M ö n i k e s ,  M . ,  Wo r t m a n n ,  F. ,  R u d o l p h-Puls, M., Kohlhase, M., Röchter, A., & Ewering, C. (2023). HUMANZENTRIERTE PRODUKTIONSPLANUNG MIT KI - Entwicklung eines Assistenzsystems. Arbeitswelt.Plus Working Paper. https://www.hsbi.de/publikationsserver/record/2855 Vo s s ,  R .  ( 2 0 2 2 ) .  Wissenschaftliches Arbeiten: Leicht verständlich! (8., überarbeitete und erweiterte Auflage). UVK Verlag. https://doi.org/10.36198/9783838588124 Willcocks, L., Lacity, M., & Craig, A. (2017). Robotic Process Automation: Strategic Transformation Lever for Global Business Services? Journal of Information Technology Teaching Cases, 7(1), 17–28. https://doi.org/10.1057/s41266-016-0016-9     III Abbildungsverzeichnis  Abbildung 1 Stufen der Aufgabenautomatisierung durch RPA und KI im Unternehmenskontext (Bulander et al., 2022) .................................................................... 5     1 1 Abstract und Keywords Der zunehmende Wettbewerbsdruck und der fortschreitende technologische Wandel zwingen Unternehmen, administrative Prozesse kontinuierlich effizienter zu gestalten. Eine vielversprechende Lösung liegt im Einsatz von KI-Agenten, welche repetitive und standardisierte Sachbearbeitungsaufgaben übernehmen und somit signifikant zur Effizienzsteigerung beitragen könnten. Diese Disposition untersucht, wie genau KI-Agenten in Unternehmen implementiert werden können, um Ressourcen einzusparen und administrative Tätigkeiten zu optimieren. Dabei werden technologische V oraussetzungen, organisatorische Herausforderungen und Effizienzpotenziale systematisch analysiert. Basierend auf aktuellen wissenschaftlichen Erkenntnissen (z.B. Bulander et al., 2022; Schönberger & Beinke, 2023) wird zunächst dargestellt, welche administrativen Aufgaben KI-Agenten besonders effektiv automatisieren können und welche V oraussetzungen dabei entscheidend sind (Pfeiffer, 2020). Anschliessend werden bestehende Forschungslücken identifiziert, insbesondere hinsichtlich der spezifischen Auswirkungen auf kleine und mittlere Unternehmen (KMU). Mithilfe eines Mixed-Methods-Forschungsansatzes werden qualitative Experteninterviews und quantitative Mitarbeitendenbefragungen kombiniert, um sowohl subjektive als auch objektiv messbare Effekte des KI-Einsatzes zu erfassen (V oss, 2022). Ziel ist es, belastbare und praxisrelevante Erkenntnisse zu gewinnen, welche Unternehmen konkret bei der Implementierung von KI-Agenten unterstützen und aufzeigen, unter welchen V oraussetzungen diese Technologien ihre grössten Potenziale entfalten können.  Keywords: Künstliche Intelligenz, Robotic Process Automation, Sachbearbeitung, Automatisierung, Effizienzsteigerung, Mixed Methods, KMU, Digitalisierung, LLMs, Akzeptanz    2 2 Problemstellung und Relevanz des Themas Die zunehmende Digitalisierung und der technologische Fortschritt stellen Unternehmen vor erhebliche Herausforderungen. Besonders administrative Prozesse, die von repetitiven und standardisierten Tätigkeiten geprägt sind, beanspruchen umfangreiche personelle Ressourcen und verursachen häufig Effizienzverluste durch menschliche Fehler. Ein Ansatz zur Lösung dieser Herausforderungen stellt der Einsatz von KI-Agenten (Künstliche Intelligenz) dar, welche speziell auf die Automatisierung einfacher Sachbearbeitungsaufgaben zugeschnitten sind (Bulander et al., 2022).  Die Relevanz dieser Technologie ist sowohl aus wissenschaftlicher als auch aus praktischer Sicht enorm. Aus wissenschaftlicher Perspektive existieren bereits empirische Untersuchungen, die das Potenzial von KI-Agenten für administrative Aufgaben belegen. So zeigen Studien, dass KI-Systeme, insbesondere in Verbindung mit Robotic Process Automation (RPA), in der Lage sind, repetitive Aufgaben mit hoher Genauigkeit und deutlich reduziertem Fehlerrisiko durchzuführen (Bulander et al., 2022).  Praktisch betrachtet bietet die Integration von KI-Agenten Unternehmen die Möglichkeit, ihre internen Ressourcen effizienter zu nutzen. Durch die Automatisierung administrativer Aufgaben können Mitarbeitende entlastet und stattdessen für komplexere, wertschöpfende Tätigkeiten eingesetzt werden. Willcocks et al. (2017, S. 19) zeigen anhand mehrerer Fallstudien, dass Robotic Process Automation (RPA) in Kombination mit KI erhebliche Produktivitätsgewinne und Qualitätsverbesserungen in der Sachbearbeitung ermöglicht. Gleichzeitig unterstützen solche Systeme die Standardisierung und Dokumentation von Wissen innerhalb der Prozesse, wodurch das Risiko von Informationsverlusten bei Personalwechseln reduziert werden kann (Willcocks et al., 2017, S. 18–19).  Trotz dieser offensichtlichen Vorteile befinden sich viele Unternehmen noch in der frühen Phase der KI-Integration. Einer Erhebung zufolge nutzen aktuell erst 5.8 % der deutschen Unternehmen KI-Technologien (Rammer et al., 2020, S. 1). Zu den grössten Hemmnissen gehören neben technischen Herausforderungen vor allem Akzeptanzprobleme seitens der Mitarbeitenden sowie der hohe Aufwand bei der Implementierung und Anpassung bestehender Prozesse (Petrat et al., 2022, S. 65; Pfeiffer, 2020, S. 467). Die Akzeptanz der Mitarbeitenden ist ein entscheidender Faktor, der massgeblich von der Transparenz der eingesetzten KI-Systeme und der Klarheit über deren unterstützende Rolle abhängt (Petrat et al., 2022, S. 65–66).     3 Zusätzlich verändern KI-Systeme auch die Rolle der Mitarbeitenden innerhalb des Unternehmens fundamental. Ihre Aufgaben verschieben sich zunehmend hin zu strategischeren und überwachenden Tätigkeiten, während operative Routinearbeiten an KI-Systeme ausgelagert werden (Schönberger & Beinke, 2023, S. 1177; Stumpp & Morelli, 2023, S. 118). Somit ergeben sich auch neue Anforderungen an die Kompetenzprofile der Mitarbeitenden, die verstärkt technologische und analytische Fähigkeiten sowie Anpassungsfähigkeit und Offenheit für neue Arbeitsweisen umfassen (Pfeiffer, 2020, S. 471–472).  Die Auseinandersetzung mit dem Thema ist somit sowohl für Unternehmen, die Wettbewerbsvorteile durch effizientere Prozessgestaltung erzielen möchten, als auch für die wissenschaftliche Forschung von hoher Relevanz. Der Bedarf',\n",
       " 'dem Thema ist somit sowohl für Unternehmen, die Wettbewerbsvorteile durch effizientere Prozessgestaltung erzielen möchten, als auch für die wissenschaftliche Forschung von hoher Relevanz. Der Bedarf an fundierter empirischer Forschung, die spezifische Implementierungserfahrungen dokumentiert und Erfolgsfaktoren klar herausarbeitet, ist dabei besonders gross.     4 3 Stand der Forschung Die zunehmende Implementierung von KI-Agenten in administrative Unternehmensprozesse hat in den letzten Jahren an Aufmerksamkeit gewonnen und ist vermehrt Gegenstand wissenschaftlicher Untersuchungen geworden. Im Zentrum bisheriger Forschungsarbeiten steht insbesondere die Analyse der Potenziale, Herausforderungen und Auswirkungen dieser Technologien in Unternehmen. 3.1 Anwendungsmöglichkeiten und Potenziale von KI-Agenten Die bestehende Forschung zeigt vielfältige Einsatzmöglichkeiten und V orteile von KI-Agenten im Unternehmenskontext auf. Insbesondere Robotic Process Automation (RPA) in Verbindung mit KI bietet das Potenzial, repetitive und strukturierte Sachbearbeitungsaufgaben effizient und qualitativ hochwertig zu automatisieren. So konnten Bulander et al. (2022) feststellen, dass entsprechende Systeme zu einer verbesserten Prozessqualität und Reduktion der Fehlerquote beitragen – vor allem bei standardisierten, wiederkehrenden Tätigkeiten. Auch Willcocks et al. (2017, S. 21) belegen anhand mehrerer Fallstudien, dass der gezielte Einsatz von RPA in administrativen Bereichen zu einer signifikanten Reduktion manueller Eingriffe führen kann. Neben der Erhöhung der Bearbeitungsgeschwindigkeit werden in diesen Studien auch geringere Fehlerraten und eine verbesserte Nachvollziehbarkeit von Prozessen hervorgehoben. Die zunehmende Automatisierung administrativer Aufgaben kann entlang unterschiedlicher Stufen beschrieben werden – vom manuellen System bis hin zum autonomen KI-basierten System. Abbildung 1 veranschaulicht diese Entwicklung und zeigt, wie RPA-Systeme – mit und ohne KI – typischerweise in Stufe 2 und 3 einzuordnen sind, wobei der Mensch sukzessive durch maschinelle Prozesse entlastet wird (Bulander et al., 2022).    5 Grosse Sprachmodelle (Large Language Models, LLMs) wie ChatGPT eröffnen darüber hinaus neue Dimensionen der Automatisierung in Bereichen wie Textverarbeitung, E-Mail-Kommunikation oder der Erstellung von Präsentationen. Studien zeigen, dass diese Systeme nicht nur Aufgaben schneller ausführen, sondern auch neue Formen der Zusammenarbeit zwischen Mensch und Maschine ermöglichen. Dadurch werden Mitarbeitende von Routinetätigkeiten entlastet und können sich verstärkt auf strategische und kreative Aufgaben konzentrieren (Schönberger & Beinke, 2023, S. 1175; Stumpp & Morelli, 2023, S. 117–118). Zusammenfassend lässt sich festhalten, dass KI-Agenten in verschiedenen administrativen Funktionsbereichen zunehmend als unterstützende Werkzeuge eingesetzt werden. Sie fördern nicht nur die Effizienz, sondern ermöglichen auch eine neue Arbeitsteilung zwischen Mensch und Maschine, bei der technologische Systeme standardisierbare Aufgaben übernehmen, während menschliche Mitarbeitende vermehrt koordinierende und entscheidungsrelevante Rollen einnehmen. 3.2 Technologische Voraussetzungen und organisatorische Herausforderungen Trotz der klar aufgezeigten Potenziale weist die Forschung ebenso auf bedeutende technologische und organisatorische Herausforderungen hin. So ist eine grundlegende Vo r a u s s e t z u n g  f ü r  e i n e  e r f o l g r e i c h e  I n t e g r a t i o n  v o n  K I-Agenten der Aufbau technischer',\n",
       " 'Abbildung 1 Stufen der Aufgabenautomatisierung durch RP A und KI im Unternehmenskontext (Bulander et al., 2022)    6 Kompetenzen im Unternehmen. Dazu gehören Kenntnisse im Bereich Programmierung, Statistik, Machine Learning und umfassendes IT-Verständnis (Pfeiffer, 2020, S. 466–467). Das Erfordernis spezifischer Fachkompetenzen bei Mitarbeitenden stellt für viele Unternehmen jedoch eine erhebliche Einstiegshürde dar, die oft umfangreiche Aus- und Weiterbildungsmassnahmen nötig macht. Organisatorische Aspekte wie die Akzeptanz von KI-Systemen durch Mitarbeitende sind ebenfalls entscheidend für den Erfolg. Petrat et al. (2022, S. 68) zeigen in einer empirischen Untersuchung, dass insbesondere bei Führungskräften und Entscheidungsträgern die Akzeptanz von KI-basierten digitalen Assistenten stark variiert und massgeblich vom empfundenen Nutzen und Vertrauen abhängt. Unternehmen, die KI erfolgreich implementieren möchten, müssen demnach eine sorgfältige Einbindung der Mitarbeitenden sicherstellen, um Ängste vor Arbeitsplatzverlust oder Überforderung effektiv zu adressieren. 3.3 Auswirkungen auf Prozesse und Rollenverteilung im Unternehmen Die Einführung von KI-Systemen führt zu tiefgreifenden Veränderungen der Arbeitsprozesse und Rollenverteilungen innerhalb der Unternehmen. Claus und Szupories (2021, S. 28–31) zeigen, dass KI-Agenten zunehmend Management- und Führungsaufgaben übernehmen können, wobei sie typischerweise eine unterstützende Rolle bei Entscheidungen einnehmen. Dies führt nicht nur zu effizienteren Abläufen, sondern verändert auch die Rolle des Mitarbeiters von ausführenden hin zu koordinierenden und überwachenden Tätigkeiten. Vo l l e n k e m p e r  e t  a l .  (V ollenkemper et al., 2023, S. 8) bestätigen diesen Befund und argumentieren, dass KI-Systeme bei der Produktionsplanung dazu beitragen, Mitarbeiter zunehmend von Routineaufgaben zu entlasten, sodass sie vermehrt strategische und kreative Aufgaben wahrnehmen können. Der Wandel von klassischen, klar definierten administrativen Aufgaben hin zu komplexeren und strategischen Tätigkeiten stellt gleichzeitig neue Anforderungen an das Qualifikationsprofil der Mitarbeitenden. 3.4 Forschungslücke Obwohl die Forschung vielfältige Einblicke in Potenziale, Anwendungsbereiche und Herausforderungen von KI-Agenten bietet, existieren nach wie vor wesentliche Lücken im Verständnis spezifischer Auswirkungen auf KMUs im administrativen Bereich. Konkret fehlt es an quantifizierten Untersuchungen, welche die erzielbaren Effizienzgewinne und    7 Ressourceneinsparungen eindeutig messen und vergleichbar machen (Rammer et al., 2020, S. 8). Zudem ist der Grad der tatsächlichen Integration und Akzeptanz in der Praxis nur unzureichend untersucht. Viele bestehende Studien konzentrieren sich auf Grossunternehmen oder spezifische technologische Lösungen und weniger auf kleinere und mittlere Unternehmen, die häufig mit knappen Ressourcen und begrenztem technologischem Know-how arbeiten. Zusätzlich gibt es bislang wenig Forschung dazu, wie genau Mitarbeitende langfristig auf den Einsatz von KI-Agenten reagieren und welche spezifischen Kompetenzen sie entwickeln müssen, um diese Technologien produktiv zu nutzen und effektiv zu steuern (Pfeiffer, 2020, S. 475; Schönberger & Beinke, 2023, S. 1183). Insbesondere empirische Studien, welche langfristige organisatorische, ökonomische und soziale Auswirkungen des KI-Einsatzes systematisch untersuchen, sind rar. Daraus resultiert eine wesentliche Forschungslücke, die durch die vorliegende Arbeit gezielt adressiert werden soll. Die geplante Untersuchung trägt dazu bei, diese Lücke zu schliessen, indem sie den konkreten Mehrwert und die Umsetzungsherausforderungen von KI-Agenten im administrativen Bereich kleinerer und mittlerer Unternehmen genauer untersucht und bewertet.    8 4 Forschungsfragen Ausgehend von der zuvor dargestellten Problemstellung sowie dem aufgezeigten Stand der Forschung ergibt sich folgende zentrale Hauptforschungsfrage: Hauptforschungsfrage: Wie können KI-Agenten einfache Sachbearbeitungsaufgaben übernehmen und Ressourcen in Unternehmen einsparen? Zur tieferen Untersuchung dieser Frage und einer klareren Strukturierung der Untersuchung werden ergänzend folgende Unterfragen formuliert: Unterfragen: 1. Technologische Voraussetzungen: „Welche technologischen Voraussetzungen und Kompetenzen sind für die erfolgreiche Integration von KI-Agenten in die Unternehmensprozesse erforderlich?“ 2. Integration und Akzeptanz: „Wie wirken sich KI-Agenten auf bestehende Unternehmensprozesse und die Akzeptanz durch Mitarbeitende aus?“ 3. Effizienz und Einsparungen: „In welchem Umfang tragen KI-Agenten zur Effizienzsteigerung und Kostenreduktion in administrativen Unternehmensprozessen bei?“     9 5 Forschungsdesign und methodisches Vorgehen Um die formulierte Forschungsfrage adäquat beantworten zu können, wird ein Mixed-Methods-Forschungsansatz gewählt. Dieser Ansatz erlaubt eine fundierte Analyse sowohl quantitativer als auch qualitativer Aspekte des Forschungsthemas. Laut V oss (2022) eignet sich ein Mixed-Methods-Ansatz besonders gut, um komplexe Zusammenhänge umfassend zu beleuchten und sowohl empirische Messgrössen als auch subjektive Einschätzungen und Erfahrungen zu berücksichtigen. 5.1 Gewählter Forschungsansatz Der Mixed-Methods-Ansatz kombiniert zunächst eine qualitative Exploration der relevanten Einflussfaktoren durch Experteninterviews mit anschliessender quantitativer Validierung durch standardisierte Befragungen. Diese Kombination ermöglicht es, einerseits detaillierte, subjektive Sichtweisen zu verstehen, und andererseits generalisierbare und messbare Ergebnisse hinsichtlich der Effizienzsteigerung und Ressourceneinsparung durch KI-Agenten zu erlangen (V oss, 2022, S. 41–44). 5.2 Untersuchungsobjekte und Stichprobenauswahl Die empirische Untersuchung fokussiert auf kleine und mittlere Unternehmen (KMU). KMU wurden gezielt ausgewählt, da sie typischerweise geringere finanzielle und personelle Ressourcen zur Verfügung haben und daher besonders von Effizienzgewinnen durch Automatisierung profitieren könnten. Insgesamt werden 8–10 KMU aus unterschiedlichen Branchen (z.B. Beratung, IT-Dienstleistungen, produzierendes Gewerbe) betrachtet, um eine Branchenvielfalt sicherzustellen und generalisierbare Erkenntnisse zu gewinnen. 5.3 Beschreibung der Datenerhebung In der ersten Phase (qualitativ) erfolgen Experteninterviews mit Entscheidungsträgern und IT-Verantwortlichen aus den ausgewählten KMU. Ziel ist es, tiefere Einsichten in technologische V oraussetzungen, organisatorische Herausforderungen sowie Akzeptanzfaktoren bei der Einführung von KI-Agenten zu gewinnen. Die Interviews werden halbstrukturiert durchgeführt, um einerseits die Vergleichbarkeit der Antworten zu gewährleisten und andererseits Raum für individuelle Perspektiven und Erfahrungen zu lassen. In der zweiten Phase (quantitativ) wird eine standardisierte Online-Befragung unter Mitarbeitenden der untersuchten Unternehmen durchgeführt, um insbesondere Aspekte wie    10 wahrgenommene Akzeptanz, tatsächliche Nutzung und subjektiv wahrgenommene Effizienzsteigerungen quantitativ zu erfassen. Diese Ergebnisse ermöglichen eine statistisch fundierte Validierung der qualitativen Erkenntnisse aus den Experteninterviews. 5.4 Operationalisierung und Messbarkeit Zur Beantwortung der Forschungsfragen werden folgende Konstrukte operationalisiert: • Technologische Voraussetzungen: Erhebung der notwendigen Kompetenzen und IT-Infrastrukturen (Likert-Skalen zur Einschätzung technischer Kompetenzanforderungen). • Integration und Akzeptanz: Bewertung der Mitarbeiterakzeptanz sowie deren Einfluss auf den Integrationserfolg (standardisierte Befragungen mittels Likert-Skala zur Zufriedenheit und Offenheit gegenüber KI-Technologien). • Effizienz und Einsparungen: Messung von Effizienzsteigerungen in administrativen Prozessen durch Vergleich quantitativer Kennzahlen vor und nach der Einführung der KI-Agenten (z.B. Zeitersparnis, reduzierte Fehlerquote, monetäre Einsparungen). Die Betrachtung der qualitativen und quantitativen Daten ermöglicht eine umfassende Analyse der komplexen Wirkungszusammenhänge des Einsatzes von KI-Agenten in KMU und liefert belastbare Ergebnisse für Praxis und Wissenschaft (V oss, 2022, S. 40–44).']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)\n",
    "# Split the extracted text into manageable chunks\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Display the resulting chunks\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 3\n",
      "Preview of the first chunk: Einsatz von KI-Agenten zur Automatisierung einfacher Sachbearbeitungsaufgaben in Unternehmen   Disposition   Eingereicht bei der ZHAW School of Management and Law (SML) Philipp Stalder   Wissenschaftl\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(\"Preview of the first chunk:\", chunks[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose an embedding model\n",
    "Use the SentenceTransfomer wrapper as we have done so far.\n",
    "Models are found here: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "or on HuggingFace.\n",
    "\n",
    "Embed the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=list(zip(chunks, chunk_embeddings)),\n",
    "    embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Index and save index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "d = chunk_embeddings.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings in FAISS index: 3\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(chunk_embeddings)\n",
    "print(\"Number of embeddings in FAISS index:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"faiss\", exist_ok=True)  # Erstellt den Ordner nur, wenn er nicht existiert\n",
    "\n",
    "\n",
    "faiss.write_index(index, \"faiss/faiss_index.index\")\n",
    "with open(\"faiss/chunks_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.save_local(\"faiss_langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Key for language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a retriever function\n",
    "\n",
    "arguments: query, k, index, chunks, embedding model\n",
    "\n",
    "return: retrieved texts, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def retrieve_documents(query, k, index, chunks, embedding_model):\n",
    "    \"\"\"\n",
    "    Retrieves documents using FAISS index\n",
    "    \n",
    "    Args:\n",
    "        query: Search string\n",
    "        k: Number of results to return\n",
    "        index: FAISS index\n",
    "        chunks: List of original text chunks\n",
    "        embedding_model: Embedding model\n",
    "    \n",
    "    Returns:\n",
    "        retrieved_texts: List of top-k document texts\n",
    "        distances: List of corresponding L2 distances\n",
    "    \"\"\"\n",
    "    # 1. Embed the query (handle both model types)\n",
    "    if hasattr(embedding_model, 'encode'):  # SentenceTransformer\n",
    "        query_embedding = embedding_model.encode([query])\n",
    "    else:  # HuggingFaceEmbeddings\n",
    "        query_embedding = np.array([embedding_model.embed_query(query)], dtype='float32')\n",
    "    \n",
    "    # Ensure correct shape (1 x embedding_dim)\n",
    "    if len(query_embedding.shape) == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "    \n",
    "    # 2. FAISS search (requires numpy array)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # 3. Get results\n",
    "    retrieved_texts = [chunks[i] for i in indices[0]]\n",
    "    return retrieved_texts, distances[0].tolist()\n",
    "\n",
    "# Usage example\n",
    "retrieved_texts, distances = retrieve_documents(\n",
    "    query=\"What is the main topic?\",\n",
    "    k=4,\n",
    "    index=index,          # Your FAISS index\n",
    "    chunks=chunks,        # Your text chunks\n",
    "    embedding_model=embeddings  # Your embedding model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build an answer function\n",
    "Build an answer function that takes a query, k, an index and the chunks.\n",
    "\n",
    "return: answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "def answer_query(query, k, index, chunks):\n",
    "    \"\"\"\n",
    "    Generates an answer to a query using retrieved documents and a Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query.\n",
    "        k (int): Number of documents to retrieve.\n",
    "        index: FAISS index for document retrieval.\n",
    "        chunks (list): List of text chunks.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer.\n",
    "    \"\"\"\n",
    "    # Retrieve documents using the retrieve function\n",
    "    retrieved_texts, _, _ = retrieve(query, k=k)\n",
    "    \n",
    "    if not retrieved_texts:\n",
    "        return \"I don't know the answer.\"\n",
    "    \n",
    "    # Combine retrieved texts into context\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # Define prompt\n",
    "    prompt = f\"\"\"\n",
    "    Answer the user's question based on the below context. If the context is not relevant to the question, say \"I don't know the answer.\"\n",
    "    Question: {query}\n",
    "    Context: {context}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load Hugging Face model and tokenizer\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize and generate answer\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=200, num_beams=5)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test your RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer: I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"Was können KI Agenten in Unternehmen bezwecken?\"\n",
    "answer = answer_query(query, 5, index, chunks)\n",
    "print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create a Rewriter\n",
    "\n",
    "Take a query and an api key for the model and rewrite the query. \n",
    "\n",
    "Rewriting a query: A Language Model is prompted to rewrite a query to better suit a task.\n",
    "\n",
    "Other Transfomrations are implemented in a similar fashion, this is just an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Globale Initialisierung des Modells und Tokenizers\n",
    "model_name = \"google/flan-t5-base\"\n",
    "global_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "global_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def rewrite_query(query):\n",
    "    \"\"\"\n",
    "    Rewrites a query to make it clearer or more suitable for a task using a Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query to be rewritten.\n",
    "\n",
    "    Returns:\n",
    "        str: The rewritten query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the prompt for rewriting the query\n",
    "        prompt = f\"\"\"\n",
    "        Rewrite the following query to make it clearer, more precise, or better suited for retrieving relevant information:\n",
    "        Original query: {query}\n",
    "        Rewritten query:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        inputs = global_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        \n",
    "        # Generate the rewritten query\n",
    "        outputs = global_model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=100,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Decode the output\n",
    "        rewritten_query = global_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return rewritten_query.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error rewriting query: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement the rewriter into your answer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "def rewrite_query(query, groq_api_key):\n",
    "    \"\"\"\n",
    "    Rewrites a query to make it clearer or more suitable for a task using the Groq API.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query to be rewritten.\n",
    "        groq_api_key (str): The Groq API key.\n",
    "\n",
    "    Returns:\n",
    "        str: The rewritten query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Groq client\n",
    "        client = Groq(api_key=groq_api_key)\n",
    "        \n",
    "        # Define the prompt for rewriting the query\n",
    "        prompt = f\"\"\"\n",
    "        Rewrite the following query to make it clearer, more precise, or better suited for retrieving relevant information:\n",
    "        Original query: {query}\n",
    "        Rewritten query:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call Groq API\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",  # Groq-Modell, kann z. B. auch \"mixtral-8x7b-32768\" sein\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that rewrites queries to improve clarity and precision.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        # Extract the rewritten query\n",
    "        rewritten_query = response.choices[0].message.content.strip()\n",
    "        return rewritten_query\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error rewriting query: {str(e)}\")\n",
    "        return query  # Fallback: Originale Query zurückgeben\n",
    "\n",
    "def answer_query_with_rewriting(query, k, index, chunks, groq_api_key):\n",
    "    \"\"\"\n",
    "    Generates an answer to a query using a rewritten query, retrieved documents, and the Groq API.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query.\n",
    "        k (int): Number of documents to retrieve.\n",
    "        index: FAISS index for document retrieval.\n",
    "        chunks (list): List of text chunks.\n",
    "        groq_api_key (str): The Groq API key.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Schritt 1: Query umformulieren\n",
    "        rewritten_query = rewrite_query(query, groq_api_key)\n",
    "        print(f\"Original Query: {query}\")\n",
    "        print(f\"Rewritten Query: {rewritten_query}\")\n",
    "        \n",
    "        # Schritt 2: Dokumente mit der umformulierten Query abrufen\n",
    "        retrieved_texts, _, _ = retrieve(query=rewritten_query, k=k)\n",
    "        \n",
    "        if not retrieved_texts:\n",
    "            return \"I don't know the answer.\"\n",
    "        \n",
    "        # Schritt 3: Kontext aus abgerufenen Texten erstellen\n",
    "        context = \"\\n\\n\".join(retrieved_texts)\n",
    "        \n",
    "        # Schritt 4: Prompt für die Antwortgenerierung\n",
    "        prompt = f\"\"\"\n",
    "        Answer the user's question based on the below context. If the context is not relevant to the question, say \"I don't know the answer.\"\n",
    "        Question: {rewritten_query}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Schritt 5: Groq-Client initialisieren\n",
    "        client = Groq(api_key=groq_api_key)\n",
    "        \n",
    "        # Schritt 6: Antwort generieren\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        # Antwort extrahieren\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        return answer\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error generating answer: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: What is the most important factor in diagnosing asthma?\n",
      "Rewritten Query: Here's a rewritten query that's clearer, more precise, and better suited for retrieving relevant information:\n",
      "\n",
      "\"Which clinical or laboratory criteria are most consistently correlated with accurate asthma diagnosis, and what is the relative importance of each factor in distinguishing asthma from other respiratory conditions?\"\n",
      "\n",
      "This rewritten query:\n",
      "\n",
      "1. Clarifies the scope: Instead of asking a broad, open-ended question, the rewritten query focuses on a specific aspect of asthma diagnosis.\n",
      "2. Specifies the goal: It clearly states the intention to identify the most\n",
      "LLM Answer: I don't know the answer. The provided context is about the implementation of KI-agents (Artificial Intelligence agents) in companies to automate administrative tasks, and it seems to be a research paper or a thesis. The query is about the clinical or laboratory criteria that are most consistently correlated with accurate asthma diagnosis, and the relative importance of each factor in distinguishing asthma from other respiratory conditions. This topic is not related to the context provided, which is about KI-agents and their implementation in companies.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the most important factor in diagnosing asthma?\"\n",
    "answer = answer_query_with_rewriting(query, 5, index, chunks, groq_api_key)\n",
    "print(\"LLM Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 .Evaluation\n",
    "\n",
    "Select random chunks from all your chunks, and generate a question to each of these chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import httpx\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_questions_for_random_chunks(chunks, groq_api_key, num_chunks=20, max_retries=3):\n",
    "    \"\"\"\n",
    "    Randomly selects a specified number of text chunks from the provided list,\n",
    "    then generates a question for each selected chunk using the Groq LLM.\n",
    "\n",
    "    Parameters:\n",
    "    - chunks (list): List of text chunks.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - num_chunks (int): Number of chunks to select randomly (default is 20).\n",
    "    - max_retries (int): Maximum number of retries for API calls (default is 3).\n",
    "\n",
    "    Returns:\n",
    "    - questions (list of tuples): Each tuple contains (chunk, generated_question).\n",
    "    \"\"\"\n",
    "    # Check if chunks list is empty or num_chunks is invalid\n",
    "    if not chunks:\n",
    "        print(\"Error: The chunks list is empty.\")\n",
    "        return []\n",
    "    if num_chunks <= 0:\n",
    "        print(\"Error: num_chunks must be positive.\")\n",
    "        return []\n",
    "    \n",
    "    # Adjust num_chunks if it exceeds the number of available chunks\n",
    "    num_chunks = min(num_chunks, len(chunks))\n",
    "    \n",
    "    # Randomly select the desired number of chunks\n",
    "    selected_chunks = random.sample(chunks, num_chunks)\n",
    "    \n",
    "    # Initialize the Groq client\n",
    "    client = Groq(api_key=groq_api_key)\n",
    "    \n",
    "    questions = []\n",
    "    for chunk in tqdm(selected_chunks):\n",
    "        # Build a prompt that asks the LLM to generate a question based on the chunk\n",
    "        prompt = (\n",
    "            \"Based on the following text, generate an insightful question that covers its key content:\\n\\n\"\n",
    "            \"Text:\\n\" + chunk + \"\\n\\n\"\n",
    "            \"Question:\"\n",
    "        )\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates insightful questions based on text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        generated_question = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Try calling the API with retry logic\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                    model=\"llama3-8b-8192\",\n",
    "                    messages=messages,\n",
    "                    temperature=0.5,\n",
    "                    max_tokens=100\n",
    "                )\n",
    "                generated_question = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the loop if successful\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred for chunk. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Error occurred: {str(e)}. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # If all attempts fail, use an error message\n",
    "        if generated_question is None:\n",
    "            generated_question = \"Error: Failed to generate question after several retries.\"\n",
    "        \n",
    "        questions.append((chunk, generated_question))\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_questions_for_random_chunks() missing 1 required positional argument: 'groq_api_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_questions_for_random_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (chunk, question) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(questions, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchunk[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated Question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_questions_for_random_chunks() missing 1 required positional argument: 'groq_api_key'"
     ]
    }
   ],
   "source": [
    "questions = generate_questions_for_random_chunks(chunks, num_chunks=5, max_retries=2)\n",
    "for idx, (chunk, question) in enumerate(questions, start=1):\n",
    "    print(f\"Chunk {idx}:\\n{chunk[:100]}...\\nGenerated Question: {question}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Test the questions with your built retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_generated_questions(question_tuples, k, index, texts, groq_api_key):\n",
    "    \"\"\"\n",
    "    For each (chunk, generated_question) tuple in the provided list, use the prebuilt\n",
    "    retrieval function to generate an answer for the generated question. The function\n",
    "    returns a list of dictionaries containing the original chunk, the generated question,\n",
    "    and the answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - question_tuples (list of tuples): Each tuple is (chunk, generated_question)\n",
    "    - k (int): Number of retrieved documents to use for answering.\n",
    "    - index: The FAISS index.\n",
    "    - texts (list): The tokenized text chunks mapping.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    \n",
    "    Returns:\n",
    "    - results (list of dict): Each dict contains 'chunk', 'question', and 'answer'.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for chunk, question in question_tuples:\n",
    "        # Use your retrieval-based answer function. Here we assume the function signature is:\n",
    "        # answer_query(query, k, index, texts, groq_api_key)\n",
    "        answer = answer_query(question, k, index, texts) #query, k, index,texts\n",
    "        results.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m answer_generated_questions(\u001b[43mquestions\u001b[49m, \u001b[38;5;241m5\u001b[39m, index, chunks, groq_api_key)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk Preview:\u001b[39m\u001b[38;5;124m\"\u001b[39m, item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m100\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'questions' is not defined"
     ]
    }
   ],
   "source": [
    "results = answer_generated_questions(questions, 5, index, chunks, groq_api_key)\n",
    "\n",
    "for item in results:\n",
    "    print(\"Chunk Preview:\", item['chunk'][:100])\n",
    "    print(\"Generated Question:\", item['question'])\n",
    "    print(\"Answer:\", item['answer'])\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate_answers_binary(results, groq_api_key, max_retries=3):\n",
    "    \"\"\"\n",
    "    Evaluates each answer in the results list using an LLM.\n",
    "    For each result (a dictionary containing 'chunk', 'question', and 'answer'),\n",
    "    it sends an evaluation prompt to the Groq LLM which outputs 1 if the answer is on point,\n",
    "    and 0 if it is missing the point.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (list of dict): Each dict must contain keys 'chunk', 'question', and 'answer'.\n",
    "    - groq_api_key (str): Your Groq API key.\n",
    "    - max_retries (int): Maximum number of retries if the API call times out.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): A dataframe containing the original chunk, question, answer, and evaluation score.\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    \n",
    "    for item in tqdm.tqdm(results, desc=\"Evaluating Answers\"):\n",
    "        # Build the evaluation prompt.\n",
    "        prompt = (\n",
    "            \"Evaluate the following answer to the given question. \"\n",
    "            \"If the answer is accurate and complete, reply with 1. \"\n",
    "            \"If the answer is inaccurate, incomplete, or otherwise not acceptable, reply with 0. \"\n",
    "            \"Do not include any extra text.\\n\\n\"\n",
    "            \"Question: \" + item['question'] + \"\\n\\n\"\n",
    "            \"Answer: \" + item['answer'] + \"\\n\\n\"\n",
    "            \"Context (original chunk): \" + item['chunk'] + \"\\n\\n\"\n",
    "            \"Evaluation (1 for good, 0 for bad):\"\n",
    "        )\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "        \n",
    "        generated_eval = None\n",
    "        attempt = 0\n",
    "        \n",
    "        # Retry logic in case of timeouts or errors.\n",
    "        while attempt < max_retries:\n",
    "            try:\n",
    "                llm_response = client.chat.completions.create(\n",
    "                    messages=messages,\n",
    "                    model=\"4o-mini\"\n",
    "                )\n",
    "                generated_eval = llm_response.choices[0].message.content.strip()\n",
    "                break  # Exit the retry loop if successful.\n",
    "            except httpx.ReadTimeout:\n",
    "                attempt += 1\n",
    "                print(f\"Timeout occurred during evaluation. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Error during evaluation: {e}. Retrying attempt {attempt}/{max_retries}...\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # If no valid evaluation was produced, default to 0.\n",
    "        if generated_eval is None:\n",
    "            generated_eval = \"0\"\n",
    "        \n",
    "        # Convert the response to an integer (1 or 0).\n",
    "        try:\n",
    "            score = int(generated_eval)\n",
    "            if score not in [0, 1]:\n",
    "                score = 0\n",
    "        except:\n",
    "            score = 0\n",
    "        \n",
    "        evaluations.append(score)\n",
    "    \n",
    "    # Add the evaluation score to each result.\n",
    "    for i, item in enumerate(results):\n",
    "        item['evaluation'] = evaluations[i]\n",
    "    \n",
    "    # Create a dataframe for manual review.\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_evaluations \u001b[38;5;241m=\u001b[39m evaluate_answers_binary(\u001b[43mresults\u001b[49m, openai_api_key)\n\u001b[1;32m      2\u001b[0m display(df_evaluations)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "df_evaluations = evaluate_answers_binary(results, openai_api_key)\n",
    "display(df_evaluations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
